# -*- coding: utf-8 -*-
"""RagChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ai2CLYCVALq2YgyiepDOwWsyqDmGpt3i
"""

!pip install faiss-cpu sentence-transformers transformers pdfplumber gradio torch

# rag_chatbot.py

import pdfplumber
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
import gradio as gr

# 1. Load & chunk the PDF
def load_and_chunk(path, chunk_size=300, overlap=50):
    text = []
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            t = page.extract_text()
            if t:
                text.append(t)
    words = " ".join(text).split()
    chunks = []
    step = chunk_size - overlap
    for i in range(0, len(words), step):
        chunk = words[i : i + chunk_size]
        if chunk:
            chunks.append(" ".join(chunk))
    return chunks

# 2. Embed all chunks once
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# 3. Build in‚Äêmemory index (normalized embeddings)
def build_index(chunks):
    embs = embedder.encode(chunks, show_progress_bar=True)
    embs = np.array(embs, dtype="float32")
    norms = np.linalg.norm(embs, axis=1, keepdims=True)
    embs = embs / norms
    return embs

# 4. Load FLAN-T5-Small for generation
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small")
model.eval()

# 5. Retrieval & generation functions
def retrieve(query, chunk_embs, chunks, k=3):
    q_emb = embedder.encode([query], convert_to_numpy=True)
    q_emb = q_emb / np.linalg.norm(q_emb, axis=1, keepdims=True)
    sims = (chunk_embs @ q_emb.T).squeeze()
    topk = np.argsort(-sims)[:k]
    return [chunks[i] for i in topk]

def generate_answer(query, chunk_embs, chunks, k=3, max_new_tokens=128):
    ctxs = retrieve(query, chunk_embs, chunks, k)
    prompt = "Answer the question based on the following contexts:\n\n" + \
             "\n\n".join(f"- {c}" for c in ctxs) + \
             f"\n\nQuestion: {query}\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt")
    if torch.cuda.is_available():
        model.to("cuda")
        inputs = {k: v.cuda() for k, v in inputs.items()}
    out = model.generate(**inputs, max_new_tokens=max_new_tokens)
    ans = tokenizer.decode(out[0], skip_special_tokens=True)
    return ans.strip()

# 6. Initialize once
PDF_PATH = r"AI Training Document.pdf"
chunks = load_and_chunk(PDF_PATH)
chunk_embs = build_index(chunks)

# 7. Gradio interface
def answer_fn(query, k):
    return generate_answer(query, chunk_embs, chunks, k=k)

demo = gr.Interface(
    fn=answer_fn,
    inputs=[
        gr.Textbox(label="Your question"),
        gr.Slider(1, 10, value=3, step=1, label="Top-k retrieval")
    ],
    outputs=gr.Textbox(label="Answer"),
    title="RAG Chatbot (FLAN-T5-Small)"
)

if __name__ == "__main__":
    demo.launch()

